---
title: "ML Final HW"
output:
  pdf_document: default
  html_document: default
date: "2022-08-03"
editor_options: 
  markdown: 
    wrap: 72
---

## Group Members: 
Meeth Yogesh Handa(mh58668), Nicolay Huarancay (nh23865), Jason Nania (jn28878), Nicole Pham-Nguyen (np9967)

## GitHub Link:
https://github.com/meeth414/Intro-to-ML-2-STA-380-Group-Submission

------------------------------------------------------------------------

# Probability Practice

## Part A

Since the problem states that the expected fraction of random clickers
is 0.3, then the fraction of random clickers is a random variable. Thus,
the probability of a random clicker is equal to the fraction of random
clickers, given that this is a uniform draw. If the fraction of random
clickers is a random variable, then the fraction of people who answered
yes is also a random variable. We can also say that the expected value
of yes is linear because the fraction of truthful clickers and random
clickers are random variables. This allows us say that the expected
probability of truthful clickers is equal to 1 - 0.3 (the expected
probability of random clickers), 0.7. Lastly, the expected probability
value of yes is 0.65, because we are certain that the probability of a
yes is 65%. Thus, we can solve the expression below, using the rule of
total probability, to find the fraction of people who are truthful
clickers.

-   $E\left[(Random Clicker)\right] = 0.3$
-   $P(Yes) = 0.65$
-   $P(No) = 0.35$

$E\left[P(Yes)\right] = P(Yes \mid Truthful Clicker) \cdot P(Truthful Clicker) \  + \  P(Yes \mid Random Clicker) \cdot \ E\left[P(Random Clicker)\right]$

$0.65 \ = (X \cdot \ 0.7) \ + \  (0.5 \cdot \ 0.3)$

$X \ = \ 71.44\%$
(Or in fraction terms = 0.7144)

## Part B

Using the rule of total probability, we were able to conclude that the
fraction of people who have the disease, given that they tested
positive, was 19.89%.

-   $P(Disease \mid Positive) \ \ = \ \ X$
-   $P(Positive \mid Disease) \ \ = \ \ 0.993$
-   $P(Negative \mid No Disease) \ \ = \ \ 0.9999$
-   $P(Positive \mid No Disease) \ \ = \ \ 0.0001$
-   $P(Disease) \ \ = \ \ 0.000025$
-   $P(No Disease) \ \ = \ \ 0.999975$

$P(Disease \mid Positive) \ \ = \ \ \frac{P(Disease) \ \cdot \ \ P(Positive \mid Disease)}{(P(Disease) \ \cdot \ \ P(Positive \mid Disease)) \ \ + \ \ (P(No Disease) \ \cdot \ \ P(Positive \mid No Disease))}$

$P(Disease \mid Positive) \ \ = \ \ \frac{(0.000025 \ \cdot \ \ 0.993)}{(0.000025 \ \cdot \ \ 0.993) \ \ + \ \ (0.999975 \ \cdot \ \ 0.0001)}$

$P(Disease \mid Positive) = 0.198882$

------------------------------------------------------------------------

# Wrangling the Billboard Top 100

```{r 2setup, echo=FALSE, warnings=FALSE, message=FALSE}

library(tidyverse)

# Loading data
billboard = read.csv('billboard.csv')

#Getting relevant columns
billboard = billboard[,c('performer','song','year','week','week_position')]

#head(billboard)

```

## Part A

**Top 10 most popular songs**

```{r 2PA, echo = FALSE, message=FALSE, warnings=FALSE, message=FALSE}

# Getting Ranking of Songs
df_ranking_songs = billboard %>%
  filter(year<2021) %>%
  group_by(performer,song) %>%
  summarize(count = n()) %>%
  arrange(desc(count))

# Getting Top 10 songs
df_top_10 = df_ranking_songs[1:10,]

# Showing Top 10 songs
df_top_10

```

**Comments**:\
The top 10 most popular songs since 1958 in the 'Billboard Top 100' are:
*Radioactive, Sail, I'm Yours, How Do I Live, Party Rock Anthem,
Counting Stars, Rolling In The Deep, Foolish Games/You Were Meant For
Me, Before He Cheats, You And Me*. The most popular song ever since 1958
has appear in the 'Billboard Top 100' up to 87 weeks.

## Part B

**Musical Diversity**

```{r 2PB.1, echo = FALSE, warnings=FALSE, message=FALSE}

# Getting unique songs per year
df_unique_songs_year = billboard %>%
  filter(year>1958&year<2021) %>%
  group_by(year) %>%
  summarize(number.unique.songs = n_distinct(song))

```

```{r 2PB.2, echo = FALSE, warnings=FALSE, message=FALSE}

# Plot
ggplot(df_unique_songs_year, aes(x=year, y=number.unique.songs)) +
  geom_line(color = 'red') +
  geom_point() + ylim(0, 1000) +
  labs(title = "Song Diversity", caption = "Based on 'Billboard Top 100'")

```

**Comments**:\
In this line graph we can observe that the number of different songs
which appear in the 'Billboard Top 100' each year had a decreasing trend
from 1966 to 2001. Less diversity means that less songs remains much
time in year in the 'Billborad Top 10'. On the other hand, this behavior
was reverse from 2001 to 2020 in which there was an increasing trend;
more diversity.

## Part C

**Performers with at least 30 10-week hits**

```{r 2PC.1, echo = FALSE, message=FALSE, warnings=FALSE}

# Getting performers with at least 30 songs with 10 week hit
df_30songs_10wh = 
billboard %>%
  group_by(performer,song) %>%
  summarize(n_weeks = n()) %>%
  filter(n_weeks >= 10) %>%
  group_by(performer) %>%
  summarize(n.songs.10w = n()) %>%
  filter(n.songs.10w >= 30) %>%
  arrange(desc(n.songs.10w))

#df_30songs_10wh

```

```{r 2PC.2, echo = FALSE, message=FALSE, warnings = FALSE}

# Plot
ggplot(df_30songs_10wh, aes(fct_reorder(performer,n.songs.10w), n.songs.10w)) +
  geom_col() +
  coord_flip() +
  labs(x="performer", y="number of 10-week hits",
       title="Performers with at least 30 10-week hits")

```

**Comments**:\
This graph show us the 19 artists in U.S. musical history since 1958 who
have had at least 30 songs that were "ten-week hits." As we can observe,
Elton John is the performer with the more number of that songs (52) and
he is followed by Madonna with 44 songs.

------------------------------------------------------------------------

# Visual story telling part 1: green buildings

```{r 3, echo=FALSE, warnings=FALSE, message=FALSE}
green = read.csv('greenbuildings.csv')
#View(green)

library(dplyr)
library(ggplot2)
# converting cluster to a factor so it can be grouped
green$green_rating = as.factor(green$green_rating)

# creating new data frame grouped by cluster and green rating, so we can compare average rents
cluster_rent <- green %>%
  group_by(cluster,green_rating) %>%
  summarize(avg_rent = mean(Rent)) %>%
  filter(avg_rent == max(avg_rent))

View(cluster_rent)

cluster_rent %>%
  group_by(green_rating) %>%
  summarize(count = n())

#View(test)

cleaned <- green %>%
  select(cluster, size, Rent, leasing_rate, age, class_a, class_b, green_rating,Gas_Costs, Electricity_Costs) %>%
  mutate(utility = Electricity_Costs + Gas_Costs) %>%
  mutate(class_type = ifelse(class_a == 1, "A", "B"))

avg_occup <- green %>%
  group_by(cluster,green_rating) %>%
  summarize(avg_lease = mean(leasing_rate)) %>%
  filter(avg_lease == max(avg_lease))

avg_occup %>%
  group_by(green_rating) %>%
  summarize(count = n()) %>%
  ggplot(aes(x = green_rating, y = count, fill = green_rating)) + geom_bar(stat = "identity") +
  scale_fill_manual(values=c("#000000","#009E73")) + ggtitle("Max Average Occupancy by Cluster") +
  xlab("Building Type") + ylab("Count of Clusters") + theme(plot.title = element_text(hjust = 0.5))

avg_cost <- cleaned %>%
  group_by(cluster,green_rating) %>%
  summarize(avg_c = mean(utility)) %>%
  filter(avg_c == min(avg_c))

avg_cost %>%
  group_by(green_rating) %>%
  summarize(count = n()) %>%
  ggplot(aes(x = green_rating, y = count, fill = green_rating)) + geom_bar(stat = "identity") +
  scale_fill_manual(values=c("#000000","#009E73")) + ggtitle("Conservative Average Profit by Cluster") +
  xlab("Building Type") + ylab("Count of Clusters") + theme(plot.title = element_text(hjust = 0.5))

View(avg_occup)

cleaned %>%
  group_by(green_rating, class_type) %>%
  summarize(count = n()) %>%
  transmute(class_type, percent = count/sum(count)) %>%
  ggplot(aes(x = class_type, y = percent, fill = green_rating)) + geom_bar(stat = "identity",                                                                        orientation = "horizontal", position = "dodge") +
  coord_flip() +
  ylim(0.0,1.0) +
  scale_fill_manual(values=c("#000000","#009E73")) + ggtitle("Green Building Percentage by Building Quality Type") +
  xlab("Building Type") + ylab("Percent") + theme(plot.title = element_text(hjust = 0.5))

```

Comments: We agree with the analyst's opinion that a green building
would be the best investment for the company. However, we disagree with
the analysis to reach that conclusion. In his analysis, he groups all of
the houses together without taking into account the different markets
across Austin. In addition, since the analysis was based solely off of the
expected numbers given, and not accounting for fluctuation, we felt that
his analysis was not every strong. Thus, we conducted our own analysis
accounting for these variables.

In plot 1, "Max Average Occupancy by Cluster," we can see that the occupancy rates across Austin are primarily in green buildings, allowing us to make the initial conclusion that Austinites tend to prefer green buildings over non-green buildings.

In plot 2, "Conservative Average Profit by Cluster," we accounted for the building types and the average profit by cluster. In order to create this plot, we assumed that all the green buildings paid for the utility costs of the tenant, to get an extremely conservative amount of profit for green buildings vs. non-green buildings. This allows us to conservatively compare the lower-end of profit landlords receive from green buildings compared to non-green buildings. Thus, the profit was calculated as: Rents received - total utility costs. Even with this conservative factor, the average profit by green buildings vastly outperformed the non-green buildings.

Finally, we wanted to evaluate the building quality, as company brand and longevity is extremely important. In plot 3, "Green Building Percentage by Building Quality Type" we looked at the percentage of green buildings vs. non-green buildings and their respective building quality types. The plot suggests that green buildings have an extremely high ratio of a label of a "good" building quality type compared to the non-green buildings.

Although we found higher utility costs associated with a few green buildings which was surprising, the average conservative profit generated by each green building cluster was still significantly higher than the non-green buildings.

Thus, we can conclude that the company should invest in a green building, despite the extra initial cost, since green buildings in this dataset lead to a higher average occupancy, a higher average profit, and a higher building quality rating.

------------------------------------------------------------------------

# Visual story telling part 2: Capital Metro data

```{r 4setup, echo=FALSE, warnings=FALSE, message=FALSE}
# Loading data
metro = read.csv('capmetro_UT.csv')

par(mfrow=c(2,2))

metro %>%
  group_by(hour_of_day, weekend) %>%
  summarize(avg_temp = mean(temperature)) %>%
  ggplot(aes(x = hour_of_day, y = avg_temp)) + geom_bar(stat = "identity", color = "dark orange") +
  ggtitle("Temperature") +
  xlab("Hour of Day") + ylab("Temp") + theme(plot.title = element_text(hjust = 0.5)) +
  facet_wrap(~weekend)

metro %>%
  ggplot(aes(x = hour_of_day, y = boarding)) + geom_bar(stat = "identity", color = "dark blue") +
  ggtitle("Boarding") +
  xlab("Hour of Day") + ylab("Count of People") + theme(plot.title = element_text(hjust = 0.5)) +
  facet_wrap(~weekend)

```

```{r 4backup, echo=FALSE, warnings=FALSE}
# metro %>%
#   ggplot(aes(x = hour_of_day, y = temperature)) + geom_line(color = "dark blue") +
#   facet_wrap(~weekend) +
#   ggtitle("Alighting") +
#   xlab("Time of Day") + ylab("Count of People") + theme(plot.title = element_text(hjust = 0.5))
# 
# metro %>%
#   filter(weekend == "weekday")%>%
# #  group_by(hour_of_day) %>%
#   ggplot(aes(x = hour_of_day) ) + 
#   geom_bar( aes(y = boarding),stat = "identity", color="dark blue") +
#   geom_line( aes(y = temperature), size = 2, color="dark orange") +
#   scale_y_continuous(
#     name = "Boarding Count",
#     sec.axis = sec_axis(~.,name = "Temperature", )
#   )
# 
# ?sec_axis()
```

Comments: 

The first plot measures the temperature vs. hour of day. We can see that the highest temperatures occur from 3-5P.M. Interestingly, this trend is also seen in the bus boarding rates across each hour, with maximum boarding rates also occurring around 3-5P.M. In order to account for E.O.D. traffic (for after-work commuters), we also looked at the weekend temperatures and the weekend boarding rates. Although not as visually significant as the weekday, this trend still occurs during the weekend.

This allows us to make the conclusion that boarding is seen to have a positive correlation with temperature because as the temperature increases during the day, so does the number of people who want to take the bus. This pattern is probably due to people taking the bus as opposed to walking or taking the car to avoid the extreme heat, as cars can overheat too or be unpleasant to ride in when hot.

------------------------------------------------------------------------

# Portfolio modeling

```{r 5, echo=FALSE, message = FALSE, warnings=FALSE, message=FALSE}


library(mosaic)
library(quantmod)
library(foreach)

# Import a few stocks
mystocks = c("AOK", "JPST", "GOVT","VBK","SCHG","DRN","SOXX")
getSymbols(mystocks)
#

# Adjust for splits and dividends
AOKa = adjustOHLC(AOK)
JPSTa = adjustOHLC(JPST)
GOVTa = adjustOHLC(GOVT)
VBKa = adjustOHLC(VBK)
SCHa = adjustOHLC(SCHG)
DRNa = adjustOHLC(DRN)
SOXXa = adjustOHLC(SOXX)

# Look at close-to-close changes
plot(ClCl(AOKa))#Nov 19 2008
plot(ClCl(JPSTa))# May 19 2017
plot(ClCl(GOVTa))# Feb 27 2017
plot(ClCl(VBKa))#Jan 03 2007
plot(ClCl(SCHa))#Jan 04 2010
plot(ClCl(DRNa))#Jul 16 2009
plot(ClCl(SOXXa))#Jan 03 2007


# Combine close to close changes in a single matrix
all_returns = cbind(ClCl(AOKa),ClCl(JPSTa),ClCl(GOVTa),ClCl(VBKa),ClCl(SCHa),ClCl(DRNa),ClCl(SOXXa))
head(all_returns)
# first row is NA because we didn't have a "before" in our data
all_returns = as.matrix(na.omit(all_returns))
N = nrow(all_returns)

# These returns can be viewed as draws from the joint distribution
# strong correlation, but certainly not Gaussian!  
pairs(all_returns)
plot(all_returns[,1], type='l')


#---------------------------------------------------------

# Set aggressive Stocks
aggressive = c("VBK","SCHG","DRN","SOXX")
aggressive_prices = getSymbols(aggressive, from = "2017-06-01")

# A chunk of code for adjusting all stocks
# creates a new object adding 'a' to the end
# For example, WMT becomes WMTa, etc
for(ticker in aggressive) {
  expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
  eval(parse(text=expr))
}

head(VBKa)

# Combine all the returns in a matrix
all_returns = cbind(	ClCl(VBKa),
                      ClCl(SCHa),
                      ClCl(DRNa),
                      ClCl(SOXXa))
head(all_returns)
all_returns = as.matrix(na.omit(all_returns))


# Now simulate many different possible futures
# just repeating the above block thousands of times
initial_wealth = 100000
sim1 = foreach(i=1:5000, .combine='rbind') %do% {
  total_wealth = initial_wealth
  weights = c(0.25,0.25,0.25, 0.25)
  holdings = weights * total_wealth
  n_days = 20
  wealthtracker = rep(0, n_days)
  for(today in 1:n_days) {
    return.today = resample(all_returns, 1, orig.ids=FALSE)
    holdings = holdings + holdings*return.today
    total_wealth = sum(holdings)
    wealthtracker[today] = total_wealth
  }
  wealthtracker
}


# each row is a simulated trajectory
# each column is a data
head(sim1)
hist(sim1[,n_days], 25)

# Profit/loss
mean(sim1[,n_days])
mean(sim1[,n_days] - initial_wealth)
hist(sim1[,n_days]- initial_wealth, breaks=30)

# 5% value at risk:
quantile(sim1[,n_days]- initial_wealth, prob=0.05)


#---------------------------------------------------------

# Set Non Aggressive Stocks
non_aggressive = c("AOK", "JPST", "GOVT")

non_aggressive_prices = getSymbols(non_aggressive, from = "2017-06-01")

# A chunk of code for adjusting all stocks
# creates a new object adding 'a' to the end
# For example, WMT becomes WMTa, etc
for(ticker in non_aggressive) {
  expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
  eval(parse(text=expr))
}


head(GOVTa)

# Combine all the returns in a matrix
all_returns = cbind(	ClCl(AOKa),
                      ClCl(JPSTa),
                      ClCl(GOVTa))

all_returns = as.matrix(na.omit(all_returns))
head(all_returns)


# Now simulate many different possible futures
# just repeating the above block thousands of times
initial_wealth = 100000
sim1 = foreach(i=1:5000, .combine='rbind') %do% {
  total_wealth = initial_wealth
  weights = c(1/3,1/3,1/3)
  holdings = weights * total_wealth
  n_days = 20
  wealthtracker = rep(0, n_days)
  for(today in 1:n_days) {
    return.today = resample(all_returns, 1, orig.ids=FALSE)
    holdings = holdings + holdings*return.today
    total_wealth = sum(holdings)
    wealthtracker[today] = total_wealth
  }
  wealthtracker
}


# each row is a simulated trajectory
# each column is a data
head(sim1)
hist(sim1[,n_days], 25)

# Profit/loss
mean(sim1[,n_days])
mean(sim1[,n_days] - initial_wealth)
hist(sim1[,n_days]- initial_wealth, breaks=30)

# 5% value at risk:
quantile(sim1[,n_days]- initial_wealth, prob=0.05)

#---------------------------------------------------------

```

Comments: We chose to have two separate portfolios based off of aggressiveness vs. non-aggressiveness. The aggressive portfolio comprised of: VBK, SCHG, DRN, and SOXX. The safe portfolio comprised of: AOK, JPST, and GOVT.

The aggressive portfolio had a calculated mean profit of $102,109.40 over the year, with an expectation to gain around 2,109 dollars per day. Using a 5% value at risk means that out of our simulations performed, 95% of them will perform better than a loss of 13,078.49 dollars over a 4-week period.

The safe portfolio had a calculated mean profit of $100,161.60 over the year, with an expectation to gain around 162 dollars per day. Using a 5% value at risk means that out of our simulations, 95% of them will perform better than a loss of 1,351.52 dollars over a 4-week period.

Out of our two portfolios, the aggressive portfolio returned a slightly higher mean profit over the year, although the daily amount gained per day vastly out-performed the safe portfolio (2,109 dollars vs. 162 dollars). Although the aggressive portfolio can incur better gains than the safe portfolio, the potential loss is also much greater than the safe portfolio (5% of simulations have a mean loss of 13,078.49 dollars vs. 1,351.52 dollars).

------------------------------------------------------------------------

# Clustering and PCA

```{r 6, echo=FALSE, warnings=FALSE, message=FALSE}
dfwine = read.csv('wine.csv',sep=",",header=TRUE)
View(dfwine)
hist(dfwine$quality)


xvar = dfwine[,c(1:11)]

xvar = scale(xvar, center=TRUE, scale=FALSE)

ii.red = dfwine$color=="red"
ii.quality = dfwine$quality > 6                        #high quality wines

pc_xvar = prcomp(xvar, rank=3)

pc_xvar$rotation

plot(pc_xvar$x[,1:2])
points(pc_xvar$x[ii.red,1:2],col="red")


plot(pc_xvar$x[,1:2])
points(pc_xvar$x[ii.quality,1:2],col="blue")

#H Clustering
wine_distance_matrix = dist(xvar, method='euclidean')


# Now run hierarchical clustering
hier_wine = hclust(wine_distance_matrix, method='complete')


# Plot the dendrogram
plot(hier_wine, cex=0.8)

# Cut the tree into 5 clusters
cluster1 = cutree(hier_wine, k=7)
summary(factor(cluster1))


  

```

Comments: In our initial analysis, we plotted the histogram of quality values for the wines in our dataset
to get an understanding of the distribution of the target variable. We then selected and scaled the
11 predictor variable values. Following that we defined the output variable values based on color (red) and quality (quality > 6 were chose as the high quality wines).

1] After the preliminary exploratory data analysis, we ran a PCA model with 2 principal components.
Observations:
1. In regards to the wine color, the majority of red wines tend to have a PC1 value < 0 whereas most of the white wines had values > 0. This is a good indicator that positive values for PC1 indicate white as the wine color.
2. For the quality of the wine, the distinction was not that clear from the plot. However, we did find that almost all he higher quality wines had PC1 values < 100.
3. Overall, the PC1 proved to be quite significant in segregating wines based on their color and to some extent, the quality as well.

2] After implementing PCA, we tried heirarchical clustering on the dataset.
Observations and Key Points:
1. We decided to proceed with 7 clusters with linkage factor as 'complete'.
2. The cluster dendogram was extremely convoluted with numerous splits suggesting that heirarchical clustering might not be the optimal choice in this test case.
3. The end results from clustering were very sensitive to outliers with only a few observations making up some of the clusters. There were no obvious actionable insights that were drawn from the results.


------------------------------------------------------------------------

# Market segmentation

```{r 7, echo=FALSE, warnings=FALSE, message=FALSE}
# Loading data
library(ggcorrplot)
library(tidyverse)
tweet <- read.csv('social_marketing.csv')

x_tweet = tweet[,c(2:37)]


pc_tweet = prcomp(x_tweet, rank=2)
pc_tweet$rotation


ggcorrplot::ggcorrplot(cor(x_tweet), hc.order = TRUE, sig.level = 0.05)


# Now run k-means clustering
kmeans_tweet = kmeans(x_tweet, 7, nstart = 25)
summary(factor(kmeans_tweet$cluster))

tweet$group = kmeans_tweet$cluster

test <- tweet %>%
  group_by(group) %>%
  summarise(across(everything(), list(mean)))

test$Max1<- colnames(test[3:38])[apply(test[3:38],1,which.max)]


comparison <- read.csv('CorrelationKmeansComparison.csv')

comparison
```

\# Correlation Plot: We began by looking at a simple correlation plot, to see if we could identify any interesting relationships between the different areas of interest. Based on our intuition and understanding of the correlation plot, we concluded there were 7 groups of correlation features that we listed below.

* Group 1 - Bots: adult and spam highly correlated 
* Group 2 - fashion, cooking, and beauty 
* Group 3 - personal fitness, health nutrition, outdoors 
* Group 4 - Parenting, religion, sports_fandom, food, school, family 
* Group 5 - college_uni, online_gaming, sports_playing
* Group 6 - politics, travel, computers 
* Group 7 - shopping, chatter, photo_sharing

K-Means: Once we established these groups we ran a k-means model to see if we would get clusters that had the same, or similar features to the ones in our initial groups. We average the different amount of tweets for each area of interest in each cluster. We then selected the 3 highest feature averages from each group. We used these three features in each to determine if the clusters determined by kmeans were similarly defined to the groups we had identified using the correlation plot. 

Result: As seen in our last data frame (comparison), 6 out of the 7 clusters seemed to have a match with our original groups.The only original group we could not find a  match for was Group 1. However, the other 6 clusters/groups are incredibly valuable because they can serve a guides for different market segments that the company can cater to. For example, Group 3 is all about being active and healthy, so that group can now be target marketed to in a way that serves those interests.


------------------------------------------------------------------------

# The Reuters corpus

```{r 8, echo=FALSE, warnings=FALSE, message=FALSE}

# Libraries
library(tm) 
library(tidyverse)
library(slam)
library(proxy)

# readerPlain
readerPlain = function(fname){
  readPlain(elem=list(content=readLines(fname)), 
            id=fname, language='en') }

## SimonCowell
file_list1 = Sys.glob('C:/Users/mailm/Documents/Intro to Machine Learning - 2/C50train/SimonCowell/*.txt')
simon = lapply(file_list1, readerPlain) 

## AaronPressman
file_list2 = Sys.glob('C:/Users/mailm/Documents/Intro to Machine Learning - 2/C50train/AaronPressman/*.txt')
aaron = lapply(file_list2, readerPlain)

## JoeOrtiz
file_list3 = Sys.glob('C:/Users/mailm/Documents/Intro to Machine Learning - 2/C50train/JoeOrtiz/*.txt')
joe = lapply(file_list3, readerPlain)

## WilliamKazer
file_list4 = Sys.glob('C:/Users/mailm/Documents/Intro to Machine Learning - 2/C50train/WilliamKazer/*.txt')
william = lapply(file_list4, readerPlain)

#--------------------------------------------------------------
# Consolidate files
file_list = c(file_list1,file_list2,file_list3,file_list4)

# Consolidate Frames
writers = rbind(simon,aaron,joe,william)
#--------------------------------------------------------------

# The file names are ugly...
#file_list

# Clean up the file names
mynames = file_list %>%
  { strsplit(., '/', fixed=TRUE) } %>%
  { lapply(., tail, n=2) } %>%
  { lapply(., paste0, collapse = '') } %>%
  unlist

# Rename the articles
#mynames
names(writers) = mynames

documents_raw = Corpus(VectorSource(writers))

my_documents = documents_raw
my_documents = tm_map(my_documents, content_transformer(tolower)) # make everything lowercase
my_documents = tm_map(my_documents, content_transformer(removeNumbers)) # remove numbers
my_documents = tm_map(my_documents, content_transformer(removePunctuation)) # remove punctuation
my_documents = tm_map(my_documents, content_transformer(stripWhitespace)) ## remove excess white-space

# Remove stopwords.
my_documents = tm_map(my_documents, content_transformer(removeWords), stopwords("en"))


## create a doc-term-matrix
DTM_writers = DocumentTermMatrix(my_documents)
DTM_writers = removeSparseTerms(DTM_writers, 0.95)

# construct TF IDF weights
tfidf_writers = weightTfIdf(DTM_writers)

#Train 
tfidf_writers_train = tfidf_writers
  
  
#-----------------------------------------------------------------------
# Test
#-----------------------------------

## SimonCowell
file_list1 = Sys.glob('C:/Users/mailm/Documents/Intro to Machine Learning - 2/C50test/SimonCowell/*.txt')
simon = lapply(file_list1, readerPlain) 

## AaronPressman
file_list2 = Sys.glob('C:/Users/mailm/Documents/Intro to Machine Learning - 2/C50test/AaronPressman/*.txt')
aaron = lapply(file_list2, readerPlain)

## JoeOrtiz
file_list3 = Sys.glob('C:/Users/mailm/Documents/Intro to Machine Learning - 2/C50test/JoeOrtiz/*.txt')
joe = lapply(file_list3, readerPlain)

## WilliamKazer
file_list4 = Sys.glob('C:/Users/mailm/Documents/Intro to Machine Learning - 2/C50test/WilliamKazer/*.txt')
william = lapply(file_list4, readerPlain)

#--------------------------------------------------------------
# Consolidate files
file_list = c(file_list1,file_list2,file_list3,file_list4)

# Consolidate Frames
writers = rbind(simon,aaron,joe,william)
#--------------------------------------------------------------

#file_list

# Clean up the file names
mynames = file_list %>%
  { strsplit(., '/', fixed=TRUE) } %>%
  { lapply(., tail, n=2) } %>%
  { lapply(., paste0, collapse = '') } %>%
  unlist

# Rename the articles
#mynames
names(writers) = mynames

documents_raw = Corpus(VectorSource(writers))

my_documents = documents_raw
my_documents = tm_map(my_documents, content_transformer(tolower)) # make everything lowercase
my_documents = tm_map(my_documents, content_transformer(removeNumbers)) # remove numbers
my_documents = tm_map(my_documents, content_transformer(removePunctuation)) # remove punctuation
my_documents = tm_map(my_documents, content_transformer(stripWhitespace)) ## remove excess white-space

# Remove stopwords.
my_documents = tm_map(my_documents, content_transformer(removeWords), stopwords("en"))


## create a doc-term-matrix
DTM_writers = DocumentTermMatrix(my_documents)
DTM_writers = removeSparseTerms(DTM_writers, 0.95)

# construct TF IDF weights
tfidf_writers = weightTfIdf(DTM_writers)

#Test
tfidf_writers_test = tfidf_writers
  


#-----------------------------------------------------------------------

#############################################
# PCA
#############################################

# Now PCA on term frequencies
X = as.matrix(tfidf_writers_train)
summary(colSums(X))
scrub_cols = which(colSums(X) == 0)
X = X[,-scrub_cols]


npca = 20
pca_writers = prcomp(X, rank=npca, scale=TRUE)

# Plot PC1 Vs PC2 (Red points: SimonCowell)
plot(pca_writers$x[,c(1,2)], xlab="PCA 1", ylab="PCA 2")
points(pca_writers$x[1:50,c(1,2)], col = 'red', pch = 18)

# Plot PC1 Vs PC3 (Red points: SimonCowell)
plot(pca_writers$x[,c(1,3)], xlab="PCA 1", ylab="PCA 3")
points(pca_writers$x[1:50,c(1,3)], col = 'red', pch = 18)


# X_test
X_test = as.matrix(tfidf_writers_test)
summary(colSums(X_test))
scrub_cols_test = which(colSums(X_test) == 0)
X_test = X_test[,-scrub_cols_test]


#----------------------------------------------------------------------

# Prediction Analysis (using PCAs as predictors)

accuracy <- data.frame(matrix(ncol = 3, nrow = 20))
colnames(accuracy) = c('NPCA','Train','Test')

for(i in 2:20){

  npca = i
  
  # Train
  # PCA
  pca_writers = prcomp(X, rank=npca, scale=TRUE)
  # Dataframe
  pca_vars = data.frame(pca_writers$x[,1:npca])
  pca_vars$target = 0
  pca_vars$target[1:50] = 1
  table(pca_vars$target)
  
  # Model
  model = glm(target ~ ., data=pca_vars, family = binomial)
  # Preds
  preds = predict(model, type = 'response')
  
  table(pca_vars$target, ifelse(preds>=preds[rank(-preds)==50],1,0) )
  accuracy_train = mean(pca_vars$target == ifelse(preds>=preds[rank(-preds)==50],1,0) )
  
  
  # Test
  # PCA
  pca_writers_test = prcomp(X_test, rank=npca, scale=TRUE)
  # Dataframe
  pca_vars_test = data.frame(pca_writers_test$x[,1:npca])
  pca_vars_test$target = 0
  pca_vars_test$target[1:50] = 1
  table(pca_vars_test$target)
  
  #Preds
  preds_test = predict(model, newdata =pca_vars_test , type = 'response')
  #
  table(pca_vars_test$target, ifelse(preds_test>=preds_test[rank(-preds_test)==50],1,0) )
  accuracy_test = mean(pca_vars_test$target == ifelse(preds_test>=preds_test[rank(-preds_test)==50],1,0) )
  
  # Results
  accuracy[i,1] = i
  accuracy[i,2] = accuracy_train
  accuracy[i,3] = accuracy_test
}

#accuracy

plot(accuracy$Train, type = 'b', col='blue',ylab='accuracy',xlab='number of PCAs',
     main = 'Accuracy in Train/Test by #PCAs')
lines(accuracy$Test, type = 'b', col='red')
legend('topleft',legend=c('Train','Test'), col=c('blue','red'), lty=c(1,1))



```
**Comments**:\
Comments:
Question: Are we able to distinguish who wrote a document based on the content and words of that document itself?

Approach: For this problem, we selected documents of four authors: Simon Cowell, Aaron Pressman, Joe Ortiz, and William Kazer, in Train and Test. So, using analytical tools, can we recognize which documents were written by Simon Cowell and which were not? First of all, we began our approach with a Text Analysis. As we learn in class, the first step is converting the unstructured data into a structured one. So, to begin we tokenized the strings and symbols, then we removed stop words and useless characters, and finally we have got DTM matrix and TFIDF weights. Now, we get our data structured.

Results: Second, to address the question above, we ran a PCA. When we plotted the first and second principal components, we observed that there is no clear pattern to distinguish which documents were written by Simon Cowell, and on the first versus third principal components plot we can't distinguish a clear pattern either. Finally, we ran Logistic regression models using the first principal components as predictors (iterating from 2 to 20 PC). We can observe that as we increase the number of principal components as predictors, the accuracy increases in Train to distinguish written documents by Simon Cowell. However, in the Test set the accuracy become flat after the 6 number of principal components, reaching 64% accuracy on average, which is quite low.

Conclusion: To conclude, after using different analytical tools to distinguish written documents, we can say that it is difficult that task to make document discrimination. However, we can highlight that although the results were not top notch, it is probable that if we extend the analysis more in deep and trying different other predictive methods and different authors, we might get better outcomes.


------------------------------------------------------------------------

# Association rule mining

```{r 9, echo=FALSE, warnings=FALSE, message=FALSE}
library(tidyverse)
library(igraph)
library(arules)
library(arulesViz)

# reading in file
groceries_raw <- read.csv('groceries.txt', sep = ",", header = FALSE)
View(groceries_raw)

# adding a row that give shopper a value from 1 to nrow
groceries_raw$shopper<-1:nrow(groceries_raw)

# converting df to correct format where each item in each persons basket has its own row
groceries_test <- groceries_raw %>% 
    pivot_longer(
        cols = c(V1, V2,V3,V4), 
        names_to = "names",
        values_to = "item")

View(groceries_test)
# removing names column
groceries <- subset(groceries_test, select = -c(names))
View(groceries)
# filling blanks with NA and
groceries[groceries==""] <- NA

groceries <- na.omit(groceries)
View(groceries)

# Top 10 most purchased items
groceries %>%
  group_by(item) %>%
  summarize(c = n()) %>%
  filter(rank(desc(c)) <= 10) %>%
  ggplot(aes(x = reorder(item,-c), y = c)) + geom_bar(stat = "identity", color = "blue", fill = "dark blue")


# Turn user into a factor
groceries$shopper = factor(groceries$shopper)

baskets = split(x=groceries$item, f=groceries$shopper)

## Remove duplicates ("de-dupe")
baskets = lapply(baskets, unique)

itemtrans = as(baskets,"transactions")
summary(itemtrans)


itemsrule = apriori(itemtrans,
                    parameter = list(support = 0.005, confidence = .1, maxlen = 4))

inspect(itemsrule)

inspect(subset(itemsrule, subset = lift > 3))
inspect(subset(itemsrule, subset = confidence > 0.3))
inspect(subset(itemsrule, subset = confidence > 0.2 & lift > 2))

plot(itemsrule, measure = c("support","lift"), shading = "confidence")

plot(itemsrule, method = 'two-key plot')

plot(itemsrule, method = "grouped", control = list(k = 50))

```

Comments: 

First, we had to do some data-wrangling in order to have the data in the correct format to be read. We then plotted the top 10 items in the dataset. After noticing we had a sizable number of each item, we proceeded to set some thresholds for our association rules.

Support: We picked a support of 0.005, because we only wanted to consider itemsets that occurred at least 50 times out of a total of 10,000 transactions in order to be confident that we have enough information to draw a conclusion from our rules.

Confidence: We chose to evaluate items with a confidence level threshold of 0.3. Although this is not extremely high, since confidence can be misleading due to association between items, we chose to have a high lift to make up for this.

Lift: We chose to have a lift threshold of 3, since a high lift means that the rise in probability of having item "Y" in the cart with the knowledge of item "X" being present over the probability of having item "Y" on the cart with no knowledge of item "X" being in the cart. Thus, having a lift of 3 guarantees that there is a strong chance that an individual will purchase item Y if they are already purchasing item "X."

Conclusions: Our association rules make sense, as those who buy meat are likely trying to cook a meal, which requires produce. In addition, people who buy drinks are more likely to purchase other types of drinks available in a grocery store. The importance of each association rule is determined by as high of a lift (the darker the red) and the size of each bubble (the support) shown above. Thus, based on our association rules, we believe that organizing a grocery store would have the best layout by organizing the whole milk next to the vegetables, all non-perishable drinks, such as soda, beer, and water, next to each other (in the same aisle), and with all meat products close to the produce, particularly beef next to citrus fruit.

